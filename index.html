<!DOCTYPE html><!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Sun Dec 05 2021 19:23:27 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="618d6acce1cb89297416119b" data-wf-site="618d6acce1cb899fe4161198">
<head>
  <meta charset="utf-8">
  <title>Music Note ID</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/music-note-id.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic","Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Domine:regular,700"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.png" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
</head>
<body class="body">
  <div data-collapse="medium" data-animation="default" data-duration="400" data-easing="ease" data-easing2="ease" role="banner" class="navigation-bar w-nav"><img src="images/ECE_horizontal_resized.png" loading="lazy" width="378" sizes="(max-width: 479px) 83vw, 378px" srcset="images/ECE_horizontal_resized-p-500.png 500w, images/ECE_horizontal_resized.png 772w" alt="">
    <div class="site-name">ECE 4554/5554</div>
  </div>
  <h1 class="heading-3">Sally Owen, Teresa Ristow, Felipe Restrepo  </h1>
  <div class="content-wrapper">
    <h1 class="heading">Course Project:<br>Music Note Identification from Spectrograms</h1>
    <div class="div-block"></div>
    <div>
      <div>
        <h1 class="heading-4">Abstract</h1>
      </div>
    </div>
    <div class="text-block">Musicians generally rely on sheet music (partiture) to interpret and play a composition of their choosing. However, in many cases, the musical composition has no readily available partiture, hence, forcing the artist to manually transcribe the song prior to playing it. While transcription &quot;by ear&quot; is commonly used amongst musicians, it is not as accurate as directly extracting the notes from the song&#x27;s spectrogram. Unfortunately, spectrograms are not easily interpretable to the human eye, as even formally trained musicians may struggle in the transcriptions process. <br><br>In this project, we tackle this issue by proposing the use of a Convolutional Neural Network (CNN) as both a spectrogram generator, and, consequently, musical note identifier/predictor. The classifier&#x27;s performance was evaluated by visually comparing the generated spectrograms against those provided in the dataset used for training and by listening to the generated and original pieces. By training and testing our proposed model on a sample of electric piano spectrograms, we demonstrate that CNN&#x27;s are capable of automating the process of music transcription, given the song&#x27;s corresponding spectrogram. </div><img src="images/Teaser_Image_5554_1.png" loading="lazy" sizes="(max-width: 479px) 73vw, (max-width: 767px) 75vw, (max-width: 991px) 76vw, (max-width: 3173px) 80vw, 2539px" srcset="images/Teaser_Image_5554_1-p-500.png 500w, images/Teaser_Image_5554_1-p-800.png 800w, images/Teaser_Image_5554_1-p-1080.png 1080w, images/Teaser_Image_5554_1-p-1600.png 1600w, images/Teaser_Image_5554_1-p-2000.png 2000w, images/Teaser_Image_5554_1.png 2539w" alt="" class="image-2"><img src="images/Screenshot-2021-12-02-203927.png" loading="lazy" sizes="(max-width: 479px) 23vw, (max-width: 991px) 24vw, 25vw" srcset="images/Screenshot-2021-12-02-203927-p-500.png 500w, images/Screenshot-2021-12-02-203927-p-800.png 800w, images/Screenshot-2021-12-02-203927.png 990w" alt="" class="image-3">
    <div>
      <h1 class="heading-5">1. Introduction</h1>
    </div>
    <div class="text-block-2">Music transcription is usually a laborious process, regardless of the employed technique. When transcribing &quot;by ear&quot;, the artist is forced to loop through the composition numerous times as they work their way through the partiture, additionally, due to the technique&#x27;s highly subjective nature, loss of accuracy is inevitable, as there is no established methodology for the process. As a result, despite the auditory nature of sound or music data, analyses with data of this nature are best done using spectrograms, i.e., graphical representations of different frequencies along with a time component, in addition to its amplitude [4]. Unfortunately, most musicians are taught how to read sheet music, but not spectrograms, as these may not be as easily interpretable to the human eye. Therefore, despite the accessibility and plethora of spectrogram data available for use online, this source of image information may not be helpful to formally trained musicians or those learning to read sheet music.<br><br>The goal of this project was to develop a machine learning algorithm that could accurately determine a composition&#x27;s sequence of notes, given excerpt&#x27;s corresponding spectrogram. Such a classifier would allow for musicians to rapidly convert spectrograms to sheet music, serving as an invaluable training tool for amateur musicians without access to their desired sheet music, and for established artists seeking to quickly convert their spectrograms to partiture. With respect to this, much of the prior research using spectrogram data focuses more on the reverse problem, that is turning sheet music into a spectrogram, for a more technology based application of the audio data [3] [5] [6]. While these applications utilize both spectrograms and music notes, our proposed project&#x27;s problem is focused more on dealing with audio data in a programing space, as we are trying to go from audio to partiture using spectrograms as our starting point. <br><br>In order to achieve this, we utilized Convolutional Neural Networks, an established deep-learning approach within the image classification and generation community. The success of CNNs in this area lies in their ability to separately convolve the input features and perform relevant feature extraction via <em>pooling</em>, eliminating the need for strict data preprocessing and extensive training times. </div>
    <div>
      <h1 class="heading-6">2. Approach</h1>
      <div class="text-block-3"><strong><em>2.1 Dataset <br></em></strong><em>‍</em>Our proposed approach utilized the Multimodal Sheet Music Dataset (MSMD) as both a starting point and source of data. The MSMD is a synthetic dataset comprised of 497 classical music pieces that contains both audio and score representations of the pieces aligned at a fine-grained level (344,742 pairs of noteheads aligned to their audio/MIDI counterpart) [5] [6].  <strong><br><br>‍<em>2.2 Preprocessing</em> <br></strong>First, we import the MSMD python library, as it allows us to properly manipulate the dataset and quickly load-in the desired data. However, once loaded, we realized that the amount of information available in the dataset far exceeded our available computational power. As a result, we decided to limit the scope of the project to electric piano compositions. To filter through available musical data, we utilized the <em>glob</em> library to cycle through the dataset&#x27;s folders and only add folders ending in <em>_electricpiano.flac_spec.npy </em>to our initial dataset. We then proceeded to extract the spectrograms and MIDI files (notes) from the selected electric piano folders, thus creating our training dataset. As a final step prior to model creation and training, we utilized a batching function (data generator) that further streamlines the training time by opening images only when required, rather than opening all images simultaneously (which would most likely overload available memory space). <strong> <br><br>‍<em>2.3 Model Training </em> <br></strong>As stated previously, we utilized a CNN to generate the predicted spectrograms, which are then compared against the original ones to assess model performance. The proposed CNN structure is the following:</div><img src="images/Screenshot-2021-12-04-234306.png" loading="lazy" sizes="(max-width: 479px) 36vw, (max-width: 991px) 38vw, 40vw" srcset="images/Screenshot-2021-12-04-234306.png 500w, images/Screenshot-2021-12-04-234306.png 741w" alt="" class="image-4"><img src="images/Screenshot-2021-12-04-234353.png" loading="lazy" sizes="(max-width: 479px) 23vw, (max-width: 991px) 24vw, 25vw" srcset="images/Screenshot-2021-12-04-234353.png 500w, images/Screenshot-2021-12-04-234353.png 800w, images/Screenshot-2021-12-04-234353.png 990w" alt="" class="image-5">
      <div class="text-block-3">Similar to prior models, we followed the standard five 1D convolution layer setup, where we use 1D convolutions instead of 2D because of the spectrograms&#x27; similarity to a time series, which are best represented by the former (1D). We also incorporate two separate up sampling layers, so that our output dimensions match those of the input image. As for our activation function, initially, we utilized a sigmoid function, however, upon further inspection of the training data, we noticed that we were dealing with an unbalanced dataset. In such a scenario, a binary cross-entropy activation function will generally outperform a sigmoid, so we opted for the use of a cross-entropy function rather than sigmoid.   As for the optimizer, we selected RMSprop as we found it worked best over other available optimizers (Adam, adabelief). <strong><br><br></strong>The model was trained over 1,000 epochs, while monitoring precision and recall as performance indicators. Regarding tuning, the number of tunable hyperparameters within a CNN exceed those that could be handled using a <em>gridsearch </em>approach, thus, we relied on the default values suggested by our referenced works and on trial and error experimentation with the number of filters present in each convolution. <strong><br><br>‍<em>2.4 Model Testing</em> <br></strong>While detailed in the section below, our model testing approach consisted  of three main parts:<br>‍<br> 1. Final (1000th epoch) classifier precision &amp; recall scores, as reported by the <em>keras</em> training process. <br>‍<br> 2. Visual inspection of the generated vs original spectrograms to ensure the model is working properly. <br>‍<br> 3. MIDI file comparison. MIDI files are treated as our &quot;ground truth&quot; as they contain the actual audibly reproducible content. Hence, once we convert our generated spectrogram to MIDI-type format, we can compare both the original and predicted compositions by listening to both files.<strong> <br><br>‍<em>2.5 Formulas</em></strong></div><img src="images/Screenshot-2021-12-05-140122.png" loading="lazy" sizes="(max-width: 479px) 36vw, (max-width: 991px) 38vw, (max-width: 2640px) 40vw, 1056px" srcset="images/Screenshot-2021-12-05-140122.png 500w, images/Screenshot-2021-12-05-140122.png 1056w" alt="" class="image-11">
      <h1 class="heading-6">3. Experiments &amp; Results</h1>
    </div>
    <div class="text-block-3">Our model was trained and evaluated using the electric piano subset we extracted from the MSMD dataset. The subset consists of 1747 electric piano samples, from which we used all (100%) for training. We used the entire dataset because of the non-traditional approach we selected for model assessment. Rather than directly testing the model&#x27; performance using unseen data (holdout set), we relied on auditory and visual feedback to evaluate how well or poorly our model did. The reasoning behind this approach is that due to music&#x27;s auditory nature, the most efficient way to determine how well the model is performing is to listen to the generated spectrogram&#x27;s MIDI file. <br><br>Strictly quantitative interpretations of model behavior may not accurately reflect how well it is performing, as music&#x27;s complex nature allows for distinct interpretations of the given piece. Moreover, since the model uses the binary cross entropy as its activation function, every note in the audio file is trying to be predicted. As the midi file size is 128 x 128, there are 16,384 predictions that the model is making for every image. Two-note, three-note, and four-note sequences are all trying to be predicted. Hence, while final accuracy/precision/recall scores may be low, once reproduced as a MIDI-type file, the spectrogram could still sound almost identical to the original piece. <br><br>Beside auditory inspection, we also employed visual inspection of the generated spectrogram against its original counterpart and monitored the model&#x27;s precision and recall scores throughout the training period. Since we used only electric piano samples, both generated and original spectrogram complexity was significantly reduced, which allowed us to use visual inspection as an additional measure of model performance. Regarding the training period, precision and recall scores at epochs [1, 500, 1000] were the following: </div><img src="images/Screenshot-2021-12-04-234445.png" loading="lazy" sizes="(max-width: 479px) 54vw, (max-width: 991px) 57vw, (max-width: 3980px) 60vw, 2388px" srcset="images/Screenshot-2021-12-04-234445.png 500w, images/Screenshot-2021-12-04-234445.png 800w, images/Screenshot-2021-12-04-234445.png 1080w, images/Screenshot-2021-12-04-234445.png 1600w, images/Screenshot-2021-12-04-234445.png 2000w, images/Screenshot-2021-12-04-234445.png 2388w" alt="" class="image-6">
    <div class="text-block-3">Final precision and recall scores were as expected. Due to the sheer volume of information contained within a spectrogram, recall scores are generally low when dealing with auditory-type data, as even slight mistakes in note-sequence predictions will cause the sequence to be &quot;incorrect&quot;, severely affecting recall scores. On the other hand, precision scores are relatively high, indicating that the model was generally successful at identifying the target notes. This can be further evidenced upon visual inspection of the generated spectrogram against the original: </div><img src="images/Screenshot-2021-12-04-234513.png" loading="lazy" sizes="(max-width: 479px) 59vw, (max-width: 767px) 61vw, (max-width: 991px) 62vw, (max-width: 3369px) 65vw, 2190px" srcset="images/Screenshot-2021-12-04-234513.png 500w, images/Screenshot-2021-12-04-234513.png 800w, images/Screenshot-2021-12-04-234513.png 1080w, images/Screenshot-2021-12-04-234513.png 1600w, images/Screenshot-2021-12-04-234513.png 2000w, images/Screenshot-2021-12-04-234513.png 2190w" alt="" class="image-7"><img src="images/Screenshot-2021-12-04-234528.png" loading="lazy" sizes="(max-width: 479px) 36vw, (max-width: 991px) 38vw, (max-width: 3322px) 40vw, 1329px" srcset="images/Screenshot-2021-12-04-234528.png 500w, images/Screenshot-2021-12-04-234528.png 800w, images/Screenshot-2021-12-04-234528.png 1080w, images/Screenshot-2021-12-04-234528.png 1329w" alt="" class="image-8"><img src="images/Screenshot-2021-12-04-234541.png" loading="lazy" sizes="(max-width: 479px) 27vw, (max-width: 767px) 28vw, (max-width: 991px) 29vw, (max-width: 3790px) 30vw, 1137px" srcset="images/Screenshot-2021-12-04-234541.png 500w, images/Screenshot-2021-12-04-234541.png 800w, images/Screenshot-2021-12-04-234541.png 1137w" alt="" class="image-9">
    <div class="text-block-3">Notice that while the initial generated spectrogram seems quite different to the original, once its values are rounded, it looks almost identical. This is because rounding &quot;separates&quot; the segments the generated spectrogram&#x27;s predicted notes, allowing for easy visual inspection against the original one. Similar to training precision and recall values, visual inspection of the spectrograms also suggests satisfactory model performance, as the generated and original spectrograms are extremely similar. Finally we evaluate the MIDI-type file of our generated spectrogram:</div><img src="images/Screenshot-2021-12-05-120851.png" loading="lazy" sizes="(max-width: 479px) 32vw, (max-width: 991px) 33vw, (max-width: 3925px) 35vw, 1374px" srcset="images/Screenshot-2021-12-05-120851.png 500w, images/Screenshot-2021-12-05-120851.png 800w, images/Screenshot-2021-12-05-120851.png 1080w, images/Screenshot-2021-12-05-120851.png 1374w" alt="" class="image-10">
    <div class="w-embed w-script">
      <div class="powr-music-player" id="a926a548_1638670756"></div>
      <script src="https://www.powr.io/powr.js?platform=webflow"></script>
    </div>
    <div class="text-block-3">Auditory assessment further corroborates that the model&#x27;s performance is satisfactory. While the audios certainly have  different pitch and tone, this can be explained by the difference in instruments used in each. Our generated audio consists of music made by an electric piano, while the original piece was most likely played on a grand piano. However, it is easily noticeable that the audio files are playing the exact same song, as both the notes and tempo played are nearly identical. </div>
    <h1 class="heading-6">4. Conclusions</h1>
    <div class="text-block-3">In this project, we successfully created a Convolutional Neural Network trained to detect the note sequences in an audio piece of music. This model can be used by musicians as an aid in transcribing musical notation from an audio clip. Traditionally, this would need to be done by ear and via multiple manual iterations through the music by a human. With the help of our model, musicians can now create a readable sheet music in a much faster and easier way.<br><br>Given more time and resources, the future of this project would be to expand testing to determine how well our model works on a variety of genres and styles of music, while incorporating additional training as necessary to further encapsulate the meters and rhythms able to be accurately perceived by the model.<br><br>Though training was limited to only the subset of electric piano pieces in the MSMD dataset due to time and resource limitations, this project could be expanded further to include other instrumentations. As opposed to experimenting with my styles and meters, a more complicated problem for which this project is a jumping off point is detecting multiple instrumental parts within one audio clip (i.e. transcription of 2 or more instrumental parts from an audio clip that has both parts playing simultaneously). </div>
    <h1 class="heading-6">References</h1>
    <div class="text-block-3">[1] Bhalke, D. G., Rao, C. R., Bormane, D. S., &amp; Vibhute, M. &quot;Spectogram based musical instrument identification using hidden markov model (HMM) for monophonic and polyphonic music signals,&quot; <em>Acta Technica Napocensis</em>, <em>52</em>(2), 1, 2011.<br>‍<br>[2] Costa, Y. M., Oliveira, L. S., &amp; Silla Jr, C. N. &quot;An evaluation of convolutional neural networks for music classification using spectrograms,&quot; <em>Applied soft computing</em>, <em>52</em>, 28-38, 2017. <br>
      <a href="https://doi.org/10.1016/j.asoc.2016.12.024">https://doi.org/10.1016/j.asoc.2016.12.024</a> <br><br>[3] Dorfer, M., Arzt, A., &amp; Widmer, G. &quot;Towards score following in sheet music images,&quot; <em>arXiv preprint arXiv:1612.05050, </em>2016. <br>
      <a href="https://arxiv.org/abs/1612.05050">https://arxiv.org/abs/1612.05050<br><br>‍</a>[4] Doshi, S. &quot;Music feature extraction in python,&quot; <em>Towards data science</em>, 30 December 2018. <br>
      <a href="https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d">https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d<br><br>‍</a>[5] Dorfer, M., Hajič Jr, J., Arzt, A., Frostel, H., &amp; Widmer, G. &quot;A multimodal audio sheet music dataset,&quot; <em>GitHub repository, </em>November 2018. <br>
      <a href="https://github.com/CPJKU/msmd">https://github.com/CPJKU/msmd<br><br>‍</a>[6] Dorfer, M., Hajič Jr, J., Arzt, A., Frostel, H., &amp; Widmer, G. &quot;Learning audio–sheet music correspondences for cross-modal retrieval and piece identification,&quot; <em>Transactions of the International Society for Music Information Retrieval</em>, <em>1</em>(1), 2018. <br>
      <a href="http://doi.org/10.5334/tismir.12">http://doi.org/10.5334/tismir.12<br><br>‍</a>[7] Guven, E., &amp; Ozbayoglu, A. M. &quot;Note and timbre classification by local features of spectrogram,&quot; <em>Procedia Computer Science</em>, <em>12</em>, 182-187, 2012. <br>
      <a href="https://doi.org/10.1016/j.procs.2012.09.051">https://doi.org/10.1016/j.procs.2012.09.051<br><br>‍</a>[8] Saha, S. &quot;A comprehensive guide to convolutional neural networks-the ELI5 way,&quot; <em>Towards Data Science </em>15 December 2018. <br>
      <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a>
    </div>
  </div>
  <footer class="footer wf-section">
    <div class="w-container">
      <div>
        <div class="w-row">
          <div class="w-col w-col-8">
            <div class="social-link-group">
              <a href="#" class="social-icon-link w-inline-block"></a>
              <a href="#" class="social-icon-link w-inline-block"></a>
              <a href="#" class="social-icon-link w-inline-block"></a>
              <a href="#" class="social-icon-link w-inline-block"></a>
              <a href="#" class="social-icon-link w-inline-block"></a>
              <a href="#" class="social-icon-link w-inline-block"></a>
            </div>
          </div>
          <div class="align-content-right w-clearfix w-col w-col-4">
            <a href="#" class="footer-link">↑ Go to top</a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <div>This is some text inside of a div block.</div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=618d6acce1cb899fe4161198" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>